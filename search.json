[{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Probability bounds of logical expressions","text":"probability calculus extension calculus propositional logic. Besides truth falsity propositions, probability calculus also considers continuum degrees credibility – probabilities – two extremes. Just propositional logic gives set rules derive truth falsity propositions truth falsity others, probability calculus gives set rules derive probability propositions probability others; rules include logical ones special case. different logical deduction systems (Pelletier & Hazen 2023), many dialects, express rules propositional logic. One sequent calculus (Takeuti 1987). probability calculus many analogies sequent calculus sequent calculus express proposition aa true within set axioms II notation ⊢\\vdash expressed probability calculus notation (note reflection) P(|)=1 \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) = 1 case can also consider degrees credibility different 11. theory “random variables” particular application probability calculus: expression “X=xX\\mathrel{\\!=\\!}x” means “quantity XX measured value xx” – just proposition. Also “causal calculus” particular application: expression “(X=x)\\mathrm{}(X \\mathrel{\\!=\\!}x)” means “quantity XX set equal xx” – also just proposition. probability calculus can fact even consider general propositions, “quantity XX reported equal xx”.  view probability calculus extension logical calculus goes back least Boole, possibly Laplace. information formalism history see e.g. Hailperin 1996, Jaynes 2003, Jeffreys 1983, Johnson 1924, Johnson 1936.","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"probabilistic-inferences","dir":"Articles","previous_headings":"","what":"Probabilistic inferences","title":"Probability bounds of logical expressions","text":"propositional logic, suppose assert : proposition aa true given set axioms II, proposition b∨¬ab \\lor \\lnot true given set axioms II augmented proposition cc. can also assert bb true given set axioms c∧Ic \\land . logical inference. notation sequent calculus (Takeuti 1987) written follows: ⊢,c∧⊢b∨¬aI∧c⊢b \\frac{ \\vdash \\quad,\\quad c \\land \\vdash b \\lor \\lnot }{ \\land c \\vdash b } conclusion follows initial assertions application set inference rules. probability calculus inference can expressed follows: P(|)=1,P(b∨¬|c∧)=1P(b|c∧)=1 \\frac{ \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) = 1\\quad,\\quad \\mathrm{P}(b \\lor \\lnot \\:\\vert\\:\\mathopen{} c \\land ) = 1 }{ \\mathrm{P}(b \\:\\vert\\:\\mathopen{} c \\land ) = 1 } final probability can shown follow initial ones well-known probability rules, valid sentences aa, bb: P(¬|)=1−P(|)\\mathrm{P}(\\lnot \\:\\vert\\:\\mathopen{} ) = 1 - \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) P(∧b|)=P(|b∧)⋅P(b|)=P(b|∧)⋅P(|)\\mathrm{P}(\\land b \\:\\vert\\:\\mathopen{} ) = \\mathrm{P}(\\:\\vert\\:\\mathopen{} b \\land )\\cdot\\mathrm{P}(b \\:\\vert\\:\\mathopen{} ) = \\mathrm{P}(b \\:\\vert\\:\\mathopen{} \\land )\\cdot\\mathrm{P}(\\:\\vert\\:\\mathopen{} ) P(∨b|)=P(|)+P(b|)−P(∧b|)\\mathrm{P}(\\lor b \\:\\vert\\:\\mathopen{} ) = \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) + \\mathrm{P}(b \\:\\vert\\:\\mathopen{} ) - \\mathrm{P}(\\land b \\:\\vert\\:\\mathopen{} ) P(|∧)=1\\mathrm{P}(\\:\\vert\\:\\mathopen{} \\land ) = 1 Another example simple probability inference, immediately follows probability rules, P(|)=0.3,P(b|∧)=0.2P(∧b|)=0.06 \\frac{ \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) = 0.3\\enspace,\\enspace \\mathrm{P}(b \\:\\vert\\:\\mathopen{} \\land ) = 0.2 }{ \\mathrm{P}(\\land b \\:\\vert\\:\\mathopen{} ) = 0.06 }  probability rules effectively imply rules propositional logic special cases. ’s remarkable probability rules allow us algorithmically determine lower upper values probability can , assertion values probabilities. well-known, instance, P(|)=0.2,P(b|)=0.7 \\mathrm{P}(\\:\\vert\\:\\mathopen{} ) = 0.2\\,,\\enspace \\mathrm{P}(b \\:\\vert\\:\\mathopen{} ) = 0.7 probability P(∧b|)\\mathrm{P}(\\land b \\:\\vert\\:\\mathopen{} ) larger minimum two , , P(∧b|)∈[0,0.2] \\mathrm{P}(\\land b \\:\\vert\\:\\mathopen{} ) \\[0, 0.2] fact also possible inequality constraints. trivial example, probability P(|)≤0.3\\mathrm{P}(\\:\\vert\\:\\mathopen{} ) \\le 0.3, must P(¬|)≥0.3\\mathrm{P}(\\lnot \\:\\vert\\:\\mathopen{} ) \\ge 0.3. Finding bounds equivalent solving linear-optimization problem. relevant texts equivalence Hailperin 1965 Hailperin 1996.","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"the-function-inferp","dir":"Articles","previous_headings":"","what":"The function inferP()","title":"Probability bounds of logical expressions","text":"function inferP() finds bounds target probability implementing linear-optimization algorithm mentioned previous section. first argument,target =, takes conditional probability two logical expressions; optional subsequent arguments takes equality inequality constraints conditional probabilities logical expressions. main notation used function (corresponding default argument solidus = TRUE) parallels mathematical notation used , following differences: : ! - : & && * : + -: > (“aa bb”, > b, simply equivalent b∨¬ab \\lor \\lnot ) additional argument solidus = FALSE given inferP(), notation used differs conditional symbol “”-connective: Conditional solidus “|{}\\:\\vert\\:\\mathopen{}{}”: ~ : + | || alternative notation allows user use proper R syntax connectives (!, &, |, &&, ||), case convenient paste R expressions. following examples main notation used.","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"simple-examples","dir":"Articles","previous_headings":"The function inferP()","what":"Simple examples","title":"Probability bounds of logical expressions","text":"Load package: examples, trivial complex. probability sentence aa can priori anything 0 1:  assume truth sentence, probability must 1:  assume negation sentence, probability must 0:  probability conditional contradictory premises, b∧¬bb \\land \\lnot b, undefined:","code":"library('Pinference') inferP(     target = P(a  |  I) ) # min max  #   0   1 inferP(     target = P(a  |  a & I) ) # min max  #   1   1 inferP(     target = P(a  |  !a & I) ) # min max  #   0   0 inferP(     target = P(a  |  b & !b & I) ) # min max  #  NA  NA"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"examples-with-constraints","dir":"Articles","previous_headings":"The function inferP()","what":"Examples with constraints","title":"Probability bounds of logical expressions","text":"statement “” “multiplication” “conjunction” rule: rule “conditional probability”: rule, inequality constraint: assert two sentences probability, sentences mutually exclusive exhaustive, must 0.5 probability: logical rule implication (modus ponens): aa ⇒ba \\Rightarrow b true, bb true: false proposition, implication always true: variant cut rule sequent calculus:","code":"inferP(     target = P(a & b  |  I),     P(a  |  I) == 0.2,     P(b  |  a & I) == 0.3 ) #  min  max  # 0.06 0.06 inferP(     target = P(b  |  a & I),     P(a & b  |  I) == 0.06,     P(a  |  I) == 0.2 ) # min max  # 0.3 0.3 inferP(     target = P(b  |  a & I),     P(a & b  |  I) == 0.06,     P(a  |  I) <= 0.2 ) # min max  # 0.3 1.0 inferP(     target = P(a  |  I),     P(a  |  I) == P(b  |  I),     P(a & b  |  I) == 0,     P(a + b  |  I) == 1 ) # min max  # 0.5 0.5 inferP(     target = P(b  |  I),     P(a  |  I) == 1,     P(a > b  |  I) == 1 ) # min max  #   1   1 inferP(     target = P(a > b  |  I),     P(a  |  I) == 0 ) # min max  #   1   1 inferP(     target = P(X + Y | I & J),     P(A & X | I) == 1,     P(Y | A & J) == 1 ) # min max  #   1   1"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"combining-evidence","dir":"Articles","previous_headings":"The function inferP()","what":"Combining evidence","title":"Probability bounds of logical expressions","text":"Suppose hypothesis HH probability 0.7 given evidence E1E_1, probability 0.8 given evidence E2E_2. probability two pieces evidence combined, , given conjunction? interesting result, proven Hailperin 2006, probability can anything:","code":"inferP(     target = P(H  |  E1 & E2 & I),     P(H  |  E1 & I) == 0.7,     P(H  |  E2 & I) == 0.8 ) # min max  #   0   1"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"the-monty-hall-problem","dir":"Articles","previous_headings":"","what":"The Monty Hall problem","title":"Probability bounds of logical expressions","text":"“Monty Hall problem”, inspired TV show Let’s make deal! hosted Monty Hall, proposed Parade magazine 1990 (see Lo Bello 1991). numbers doors changed ): Suppose game show given choice three doors. Behind one car; behind others goats. pick door . 1, host, knows behind wouldn’t open door car, opens . 3, goat. asks want pick . 2. switch? can solve problem inferP().","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"propositions","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Propositions","title":"Probability bounds of logical expressions","text":"First let’s introduce notation relevant propositions: : proposition expressing rules game, car1: “car behind door . 1”, you1: “pick door . 1”, host1: “host opens door . 1”, similarly doors.","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"target-probability","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Target probability","title":"Probability bounds of logical expressions","text":"want know ’s probability car behind door, . 2. conditional target probability , besides game rules II, three propositions expressing information : chose door . 1, you1, host opened door . 3, host3, car behind door . 3, !car3. target probability thus P(car2  |  you1 & host3 & !car3 & )","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"given-probabilities","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Given probabilities","title":"Probability bounds of logical expressions","text":"must express information game rules terms probabilities. Note several equivalent ways express probabilistic constraints listed . know one car, must behind one door. corresponds four probability constraints:P(car1 & car2  |  ) == 0P(car1 & car3  |  ) == 0P(car2 & car3  |  ) == 0P(car1 + car2 + car3  |  ) == 1 host must open one door, one door. host open door chose, open door car, must open one door. corresponds seven probability constraints:P(host1 & host2  |  ) == 0P(host1 & host3 |  ) == 0P(host2 & host3  |  ) == 0P(host1 + host2 + host3  |  ) == 1P(host1  |  you1 & ) == 0P(host2  |  car2 & ) == 0P(host3  |  car3 & ) == 0 Let’s say initially equally undecided among three doors:P(car1  |  ) == P(car2  |  )P(car2  |  ) == P(car3  |  ) fact, alone, picked door . 1 irrelevant knowledge car’s position:P(car1  |  you1 & ) == P(car2  |  you1 & )P(car2  |  you1 & ) == P(car3  |  you1 & ) Finally, reason believe host favours one door another, choice possible, , car behind door chose:P(host2  |  you1 & car1 & ) == P(host3  |  you1 & car1 & )","code":""},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"result","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Result","title":"Probability bounds of logical expressions","text":"can finally input desired target constraints inferP(): well-known result 2/3. can also check obvious result car3, 0.","code":"inferP(     target = P(car2  |  you1 & host3 & !car3 & I),     ##     P(car1 & car2  |  I) == 0,     P(car1 & car3  |  I) == 0,     P(car2 & car3  |  I) == 0,     P(car1 + car2 + car3  |  I) == 1,     ##     P(host1 & host2 | I) == 0,     P(host1 & host3 | I) == 0,     P(host2 & host3 | I) == 0,     P(host1 + host2 + host3  |  I) == 1,     P(host1  |  you1 & I) == 0,     P(host2  |  car2 & I) == 0,     P(host3  |  car3 & I) == 0,     ##     P(car1  |  I) == P(car2  |  I),     P(car2  |  I) == P(car3  |  I),     ##     P(car1  |  you1 & I) == P(car2  |  you1 & I),     P(car2  |  you1 & I) == P(car3  |  you1 & I),     ##     P(host2  |  you1 & car1 & I) == P(host3  |  you1 & car1 & I) ) #      min      max  # 0.666667 0.666667"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"omitting-some-probability-constraints","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Omitting some probability constraints","title":"Probability bounds of logical expressions","text":"interesting see happens bounds target probability constraints omitted. Suppose instance omit constraintP(host2  |  you1 & car1 & ) == P(host3  |  you1 & car1 & ) states reason believe host choose door . 2 . 3 vice versa, car behind door . 1. bounds target probability become case constraints determine single value probability car behind door . 2, still constrain 1/2. conclusion still best switch door (want car), credibility winning 2/3.","code":"inferP(     target = P(car2  |  you1 & host3 & !car3 & I),     ##     P(car1 & car2  |  I) == 0,     P(car1 & car3  |  I) == 0,     P(car2 & car3  |  I) == 0,     P(car1 + car2 + car3  |  I) == 1,     ##     P(host1 & host2 | I) == 0,     P(host1 & host3 | I) == 0,     P(host2 & host3 | I) == 0,     P(host1 + host2 + host3  |  I) == 1,     P(host1  |  you1 & I) == 0,     P(host2  |  car2 & I) == 0,     P(host3  |  car3 & I) == 0,     ##     P(car1  |  I) == P(car2  |  I),     P(car2  |  I) == P(car3  |  I),     ##     P(car1  |  you1 & I) == P(car2  |  you1 & I),     P(car2  |  you1 & I) == P(car3  |  you1 & I)     ##     ## P(host2  |  you1 & car1 & I) == P(host3  |  you1 & car1 & I) # omitted ) # min max  # 0.5 1.0"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"variations","dir":"Articles","previous_headings":"The Monty Hall problem","what":"Variations","title":"Probability bounds of logical expressions","text":"internet offers many variations Monty Hall problem. can also solved inferP(). Let’s take “Monty Fall” variation example. possibility host falls going open door. represent proposition fall. hosts falls, non-zero probability, let’s say 10%, host open door car choose door . 1. can express constraintsP(host2  |  car2 & fall & you1 & ) == 0.1P(host3  |  car3 & fall & you1 & ) == 0.1 Finally still reason believe falling host favours one door another, car behind door chose:P(host2  |  you1 & car1 & fall & ) == P(host3  |  you1 & car1 & fall & ) target probability car behind door . 2, given chose . 1, host fell opened . 3, car :P(car2  |  you1 & host3 & !car3 & fall & ). result roughly 64% probability car behind door . 2. can check probability becomes 50% ’s 50% probability falling host opens door car. interesting aspects variations precisely translate different verbal information formal propositions probability constraints.","code":"inferP(    target = P(car2  |  you1 & host3 & !car3 & fall & I),     ##     P(car1 & car2  |  I) == 0,     P(car1 & car3  |  I) == 0,     P(car2 & car3  |  I) == 0,     P(car1 + car2 + car3  |  I) == 1,     ##     P(host1 & host2 | I) == 0,     P(host1 & host3 | I) == 0,     P(host2 & host3 | I) == 0,     P(host1 + host2 + host3  |  I) == 1,     P(host1  |  you1 & I) == 0,     P(host2  |  car2 & fall & you1 & I) == 0.1,     P(host3  |  car3 & fall & you1 & I) == 0.1,     ##     P(car1  |  I) == P(car2  |  I),     P(car2  |  I) == P(car3  |  I),     ##     P(car1  |  you1 & I) == P(car2  |  you1 & I),     P(car2  |  you1 & I) == P(car3  |  you1 & I),     P(car1  |  you1 & fall & I) == P(car2  |  you1 & fall & I),     P(car2  |  you1 & fall & I) == P(car3  |  you1 & fall & I),     ##     P(host2  |  you1 & car1 & fall & I) == P(host3  |  you1 & car1 & fall & I) ) #      min      max  # 0.642857 0.642857"},{"path":"https://pglpm.github.io/Pinference/articles/inferP.html","id":"references","dir":"Articles","previous_headings":"","what":"References","title":"Probability bounds of logical expressions","text":"Jaynes (2003): Probability Theory: Logic Science (Cambridge University Press) doi:10.1017/CBO9780511790423. Jeffreys (1983): Theory Probability (3rd ed. Oxford University Press) doi:10.1093/oso/9780198503682.001.0001. Johnson (1924): Logic. Part III: Logical Foundations Science (Cambridge University Press) https://archive.org/details/logic03john. Johnson (1932): Probability: Relations Proposal Supposal (Mind 41(161):1) doi:10.1093/mind/XLI.161.1. Hailperin (1965): Best Possible Inequalities Probability Logical Function Events (. Math. Monthly 72(4):343) doi:10.1080/00029890.1965.11970533. Hailperin (1996): Sentential Probability Logic: Origins, Development, Current Status, Technical Applications (Associated University Presses) https://archive.org/details/hailperin1996-Sentential_probability_logic/. Hailperin (2006): Probability logic combining evidence (Hist. Philos. Log. 27(3):249) doi:10.1080/01445340600616289. Pelletier, Hazen 2023: Natural Deduction Systems Logic (Stanford Encyclopedia Philosophy) https://plato.stanford.edu/archives/spr2023/entries/natural-deduction/. Takeuti (1987): Proof Theory (2nd ed. North-Holland).","code":""},{"path":"https://pglpm.github.io/Pinference/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"PierGianLuca Porta Mana. Author, maintainer, copyright holder.","code":""},{"path":"https://pglpm.github.io/Pinference/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Porta Mana P (2025). Pinference: Probability Inference Propositional Logic. R package version 0.2.5, https://pglpm.github.io/Pinference/.","code":"@Manual{,   title = {Pinference: Probability Inference for Propositional Logic},   author = {PierGianLuca {Porta Mana}},   year = {2025},   note = {R package version 0.2.5},   url = {https://pglpm.github.io/Pinference/}, }"},{"path":[]},{"path":"https://pglpm.github.io/Pinference/index.html","id":"an-explanation-of-what-the-package-is-about","dir":"","previous_headings":"","what":"An explanation of what the package is about","title":"Probability Inference for Propositional Logic","text":"[explanation taken vignette accompanying package.] probability calculus extension calculus propositional logic. Besides truth falsity propositions, probability calculus also considers continuum degrees credibility – probabilities – two extremes. Just propositional logic gives set rules derive truth falsity propositions truth falsity others, probability calculus gives set rules derive probability propositions probability others; rules include logical ones special case. different logical deduction systems, many dialects, express rules propositional logic. One sequent calculus. probability calculus many analogies sequent calculus. sequent calculus express proposition aa true within set axioms II notation ⊢\\vdash expressed probability calculus notation (note reflection) P(|)=1 \\mathrm{P}(\\ \\vert\\  ) = 1 case can also consider degrees credibility different 11 (see vignette references). inferences? propositional logic, suppose assert : proposition aa true given set axioms II, proposition b∨¬ab \\lor \\lnot true given set axioms II augmented proposition cc. can also assert bb true given set axioms c∧Ic \\land . logical inference. notation sequent calculus (Takeuti 1987) written follows: ⊢ac∧⊢b∨¬aI∧c⊢b \\frac{ \\vdash \\quad\\quad c \\land \\vdash b \\lor \\lnot }{ \\land c \\vdash b } conclusion follows initial assertions application set inference rules. probability calculus inference can expressed follows: P(|)=1P(b∨¬|c∧)=1P(b|c∧)=1 \\frac{ \\mathrm{P}(\\ \\vert\\  ) = 1\\quad\\quad \\mathrm{P}(b \\lor \\lnot \\ \\vert\\  c \\land ) = 1 }{ \\mathrm{P}(b \\ \\vert\\  c \\land ) = 1 } final probability can shown follow initial ones well-known probability rules. Another example simple probability inference, immediately follows probability rules, P(|)=0.3,P(b|∧)=0.2P(∧b|)=0.06 \\frac{ \\mathrm{P}(\\ \\vert\\  ) = 0.3\\quad,\\quad \\mathrm{P}(b \\ \\vert\\  \\land ) = 0.2 }{ \\mathrm{P}(\\land b \\ \\vert\\  ) = 0.06 }  probability rules effectively imply rules propositional logic special cases. ’s remarkable probability rules allow us algorithmically determine lower upper values probability can , assertion values probabilities. algorithm equivalent solving (fractional)-linear optimization problem. well-known, instance, P(|)=0.2P(b|)=0.7 \\mathrm{P}(\\ \\vert\\  ) = 0.2 \\qquad \\mathrm{P}(b \\ \\vert\\  ) = 0.7 probability P(∧b|)\\mathrm{P}(\\land b \\ \\vert\\  ) larger minimum two , , P(∧b|)∈[0,0.2] \\mathrm{P}(\\land b \\ \\vert\\  ) \\[0, 0.2] present package offers function inferP(), implements algorithm.","code":""},{"path":"https://pglpm.github.io/Pinference/index.html","id":"examples","dir":"","previous_headings":"","what":"Examples","title":"Probability Inference for Propositional Logic","text":"Let’s use inferP() simple example just discussed: answer obtain Now let’s check classical logical inference, modus ponens. symbol > stands -⇒\\Rightarrow: example: probability proposition based contradictory premises (b∧¬bb \\land \\lnot b) undefined: Let’s check variant cut rule sequent calculus: information notation constraints available help function help('inferP'). interesting examples, Monty Hall problem, given package’s vignette.","code":"inferP(   target = P(a & b  |  c),   P(a  |  c) == 0.2,   P(b  |  c) == 0.7 ) min  max 0.0  0.2 inferP(   target = P(b | I),   P(a > b | I) == 1,   P(a | I) == 1 )  min max   1   1 inferP(   target = P(a  |  b & !b) )  min max  NA  NA inferP(   target = P(X + Y | I & J),   P(A & X | I) == 1,   P(Y | A & J) == 1 )  min max   1   1"},{"path":"https://pglpm.github.io/Pinference/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Probability Inference for Propositional Logic","text":"package can installed requires lpSolve package.","code":"remotes::install_github('pglpm/Pinference')"},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":null,"dir":"Reference","previous_headings":"","what":"Calculate lower and upper probability bounds — inferP","title":"Calculate lower and upper probability bounds — inferP","text":"inferP() calculates minimum maximum allowed values probability propositional-logic expression conditional another one, given numerical equality constraints conditional probabilities propositional-logic expressions.","code":""},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Calculate lower and upper probability bounds — inferP","text":"","code":"inferP(target, ..., solidus = TRUE)"},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Calculate lower and upper probability bounds — inferP","text":"target target probability expression (see Details). ... Probability constraints (see Details). solidus logical. TRUE (default), symbol | used introduce conditional probability; case use || ''-connective lead error. FALSE, symbol ~ used introduce conditional; case symbols |, || can used '`-connective.","code":""},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Calculate lower and upper probability bounds — inferP","text":"vector min max values target probability, NA constraints mutually contradictory. min max 0 1 constraints restrict target probability way.","code":""},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Calculate lower and upper probability bounds — inferP","text":"function takes first argument probability logical expression, conditional another expression, subsequent (optional) arguments constraints probabilities logical expressions. Propositional logic intended . function uses lpSolve::lp() function lpSolve package.","code":""},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"logical-expressions","dir":"Reference","previous_headings":"","what":"Logical expressions","title":"Calculate lower and upper probability bounds — inferP","text":"propositional-logic expression combination atomic propositions means logical connectives. Atomic propositions can name satisfies R syntax object names. Examples:   Available logical connectives \"\" (negation, \"\\(\\lnot\\)\"), \"\" (conjunction, \"\\(\\land\\)\"), \"\" (disjunction, \"\\(\\lor\\)\"), \"-\" (implication, \"\\(\\Rightarrow\\)\"). first three follow standard R syntax logical operators (see base::logical): :  !  - : & && * : +; argument solidus = FALSE, also || | allowed. \"-\" connective represented infix operator >; internally x > y simply defined x -y. Examples logical expressions:","code":"a A hypothesis1 coin.lands.tails coin_lands_heads `tomorrow it rains` # note the backticks a a & b (a + hypothesis1) & -A red.ball & ((a > !b) + c)"},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"probabilities-of-logical-expressions","dir":"Reference","previous_headings":"","what":"Probabilities of logical expressions","title":"Calculate lower and upper probability bounds — inferP","text":"probability expression \\(X\\) conditional expression \\(Y\\)entered syntax similar common mathematical notation \\(\\mathrm{P}(X \\vert Y)\\). solidus \"|\" used separate conditional (note usual R syntax symbol stands logical \"\" instead). argument solidus = FALSE given function, tilde  \"~\" used instead solidus (note usual R syntax symbol introduces formula instead). instance \\(\\mathrm{P}(\\lnot \\lor b \\:\\vert\\: c \\land H)\\) can entered following ways, among others (extra spaces added just clarity):   , argument  solidus = FALSE, following ways:   also possible use p Pr pr instead P.","code":"P(!a + b  |  c & H) P(-a + b  |  c && H) P(!a + b  |  c * H) P(!a | b  ~  c & H) P(-a + b  ~  c && H) P(!a || b  ~  c * H)"},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"probability-constraints","dir":"Reference","previous_headings":"","what":"Probability constraints","title":"Calculate lower and upper probability bounds — inferP","text":"probability constraint can one four forms:   X, Y, Z logical expressions. Note conditionals left right sides must . Inequalities <= >= also allowed instead equalities. See accompanying vignette interesting examples.","code":"P(X | Z) = [number between 0 and 1]  P(X | Z) = P(Y | Z)  P(X | Z) = P(Y | Z) * [positive number]  P(X | Z) = P(Y | Z) / [positive number]"},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Calculate lower and upper probability bounds — inferP","text":"T. Hailperin: Best Possible Inequalities Probability Logical Function Events. . Math. Monthly 72(4):343, 1965 doi:10.1080/00029890.1965.11970533. T. Hailperin: Sentential Probability Logic: Origins, Development, Current Status, Technical Applications. Associated University Presses, 1996 https://archive.org/details/hailperin1996-Sentential_probability_logic/.","code":""},{"path":"https://pglpm.github.io/Pinference/reference/inferP.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Calculate lower and upper probability bounds — inferP","text":"","code":"## No constraints inferP(   target = P(a | h) ) #> min max  #>   0   1   ## Trivial example with inequality constraint inferP(   target = P(a | h),   P(!a | h) >= 0.2 ) #> min max  #> 0.0 0.8   #' ## The probability of an \"and\" is always less ## than the probabilities of the and-ed propositions: inferP(   target = P(a & b | h),   P(a | h) == 0.3,   P(b | h) == 0.6 ) #> min max  #> 0.0 0.3   ## P(a & b | h) is completely determined ## by P(a | h) and P(b | a & h): inferP(     target = P(a & b | h),     P(a | h) == 0.3,     P(b | a & h) == 0.2 ) #>  min  max  #> 0.06 0.06   ## Logical implication (modus ponens) inferP(   target = P(b | I),   P(a | I) == 1,   P(a > b | I) == 1 ) #> min max  #>   1   1   ## Cut rule of sequent calculus inferP(   target = P(X + Y | I & J),   P(A & X | I) == 1,   P(Y | A & J) == 1 ) #> min max  #>   1   1   ## Solution to the Monty Hall problem (see accompanying vignette): inferP(     target = P(car2  |  you1 & host3 & I),     ##     P(car1 & car2  |  I) == 0,     P(car1 & car3  |  I) == 0,     P(car2 & car3  |  I) == 0,     P(car1 + car2 + car3  |  I) == 1,     P(host1 & host2 | I) == 0,     P(host1 & host3 | I) == 0,     P(host2 & host3 | I) == 0,     P(host1 + host2 + host3  |  I) == 1,     P(host1  |  you1 & I) == 0,     P(host2  |  car2 & I) == 0,     P(host3  |  car3 & I) == 0,     P(car1  |  I) == P(car2  |  I),     P(car2  |  I) == P(car3  |  I),     P(car1  |  you1 & I) == P(car2  |  you1 & I),     P(car2  |  you1 & I) == P(car3  |  you1 & I),     P(host2  |  you1 & car1 & I) == P(host3  |  you1 & car1 & I) ) #>       min       max  #> 0.6666667 0.6666667"},{"path":"https://pglpm.github.io/Pinference/news/index.html","id":"pinference-010","dir":"Changelog","previous_headings":"","what":"Pinference 0.1.0","title":"Pinference 0.1.0","text":"Initial CRAN submission.","code":""}]
